chaîne_de_caractères_PH_V
PSH5_propre <- readLines("../../Textes/Pseudo-Hegesippe/PSH_V_propre_propre.txt")
Nb_sequences <- 10
Extraits_PH_V<- strwrap(PSH5_propre, nchar(PSH5_propre) / Nb_sequences)
#On peut afficher le contenu de chaque séquence :
Extraits_PH_V[1]
if(!require("tm")){
install.packages("tm")
library("tm")
}
if(!require("tidytext")){
install.packages("tidytext")
library("tidytext")
}
#Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des données et des métadonnées.
#La fonction `VectorSource` transforme chaque document en vecteur.
corpus_PH_V <- Corpus(VectorSource(Extraits_PH_V), readerControl = list(language = "lat"))
# J'affiche les informations à propos de ce corpus
corpus_PH_V
dtm_PH_V <- DocumentTermMatrix(corpus_PH_V)
dtm_PH_V
if(!require("tm")){
install.packages("tm")
library("tm")
}
if(!require("tidytext")){
install.packages("tidytext")
library("tidytext")
}
#Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des données et des métadonnées.
#La fonction `VectorSource` transforme chaque document en vecteur.
corpus_PH_V <- Corpus(VectorSource(Extraits_PH_V), readerControl = list(language = "lat"))
# J'affiche les informations à propos de ce corpus
corpus_PH_V
dtm_PH_V <- DocumentTermMatrix(corpus_PH_V)
dtm_PH_V
freq_PH_V <- as.data.frame(colSums(as.matrix(dtm_PH_V)))
colnames(freq_PH_V) <- c("frequence")
#as.data.frame est une fonction vérifiant qu'un objet est un dataframe ou le forçant à le devenir si c'est possible.
#colSums est une fonction permettant de former des sommes et des moyennes de lignes et de colonnes pour des tableaux et des dataframes.
#as.matrix est une fonction générique convertissant en matrice.
#colnames récupère ou définit le nom des lignes et des colonnes dans un objet de type matrice.
#c est une fonction générique qui combine ses arguments. La méthode par défaut combine les arguments pour former un vecteur.
#Pour dessiner un graphe, nécessité d'installer une nouvelle library: `ggplot2`
#gg = Grammar of Graphics
#Avec ggplot 2, les données représentées graphiquement proviennent toujours d'un dataframe.
if (!require("ggplot2")){
install.packages("ggplot2")
library("ggplot2")
}
#Dessin du graphe
#La fonction ggplot initialise le graphique. On commence par définir la source des données (ici freq_Vulgate), puis on indique quelle donnée on veut représenter (les attributs esthétiques) en passant des arguments dans la fonction aes(). Cette fonction spécifie les variables à visualiser et associe à chaque variable un emplacement ou un rôle: on renseigne le paramètre x qui est la variable à représenter sur l'axe horizontal (ici la fréquence).
#On ajoute, enfin, les éléments de représentation graphique (= geom). On les ajoute à l'objet graphique de base avec l'opérateur +. geom_density permet d'afficher l'estimation de densité d'une variable numérique. On crée une courbe de distribution.
#Source de la plupart des explications : https://juba.github.io/tidyverse/08-ggplot2.html
ggplot(freq_PH_V, aes(x=frequence)) + geom_density()
motsPeuFrequents_PH_V <- findFreqTerms(dtm_PH_V, 0, 99)
length(motsPeuFrequents_PH_V)
head(motsPeuFrequents_PH_V,50)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe")
monDossier="~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe"
StopWords <- "../../StopwordsLatin(actualise).txt"
Stops = read.csv(StopWords, header=FALSE, stringsAsFactors=FALSE)[,]
head(Stops,10)
df_PH_V <- read.csv("../../Clearer_PH/PH-V-clearer-pie.csv", sep=",")
#Création d'une chaîne de caractère vide qui contiendra à l'avenir tous les textes contenus dans df.
#[Dans le cours de Simon, la colomme Lemma est déjà une chaîne de caractères. On ruse donc pour obtenir cette chaîne (elle est vide pour l'instant mais on dira à R : si le mot n'apparaît pas dans les stopwords ou dans la ponctuation, tu le mets dans cette chaîne de caractères).]
chaîne_de_caractères_PH_V <- ""
#Laisser le contenu vide : il n'y a rien pour l'instant.
for (word in tolower(df_PH_V$lemma)) {
if (!word %in% Stops) {
chaîne_de_caractères_PH_V <- paste(chaîne_de_caractères_PH_V, word, sep=" ")
}
}
chaîne_de_caractères_PH_V
PSH5_propre <- readLines("../../Textes/Pseudo-Hegesippe/PSH_V_propre_propre.txt")
Nb_sequences <- 10
Extraits_PH_V<- strwrap(PSH5_propre, nchar(PSH5_propre) / Nb_sequences)
#On peut afficher le contenu de chaque séquence :
Extraits_PH_V[1]
if(!require("tm")){
install.packages("tm")
library("tm")
}
if(!require("tidytext")){
install.packages("tidytext")
library("tidytext")
}
#Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des données et des métadonnées.
#La fonction `VectorSource` transforme chaque document en vecteur.
corpus_PH_V <- Corpus(VectorSource(Extraits_PH_V), readerControl = list(language = "lat"))
# J'affiche les informations à propos de ce corpus
corpus_PH_V
dtm_PH_V <- DocumentTermMatrix(corpus_PH_V)
dtm_PH_V
freq_PH_V <- as.data.frame(colSums(as.matrix(dtm_PH_V)))
colnames(freq_PH_V) <- c("frequence")
#as.data.frame est une fonction vérifiant qu'un objet est un dataframe ou le forçant à le devenir si c'est possible.
#colSums est une fonction permettant de former des sommes et des moyennes de lignes et de colonnes pour des tableaux et des dataframes.
#as.matrix est une fonction générique convertissant en matrice.
#colnames récupère ou définit le nom des lignes et des colonnes dans un objet de type matrice.
#c est une fonction générique qui combine ses arguments. La méthode par défaut combine les arguments pour former un vecteur.
#Pour dessiner un graphe, nécessité d'installer une nouvelle library: `ggplot2`
#gg = Grammar of Graphics
#Avec ggplot 2, les données représentées graphiquement proviennent toujours d'un dataframe.
if (!require("ggplot2")){
install.packages("ggplot2")
library("ggplot2")
}
#Dessin du graphe
#La fonction ggplot initialise le graphique. On commence par définir la source des données (ici freq_Vulgate), puis on indique quelle donnée on veut représenter (les attributs esthétiques) en passant des arguments dans la fonction aes(). Cette fonction spécifie les variables à visualiser et associe à chaque variable un emplacement ou un rôle: on renseigne le paramètre x qui est la variable à représenter sur l'axe horizontal (ici la fréquence).
#On ajoute, enfin, les éléments de représentation graphique (= geom). On les ajoute à l'objet graphique de base avec l'opérateur +. geom_density permet d'afficher l'estimation de densité d'une variable numérique. On crée une courbe de distribution.
#Source de la plupart des explications : https://juba.github.io/tidyverse/08-ggplot2.html
ggplot(freq_PH_V, aes(x=frequence)) + geom_density()
motsPeuFrequents_PH_V <- findFreqTerms(dtm_PH_V, 0, 99)
length(motsPeuFrequents_PH_V)
head(motsPeuFrequents_PH_V,50)
motsTresFrequents_PH_V <- findFreqTerms(dtm_PH_V, 99, Inf)
length(motsTresFrequents_PH_V)
head(motsTresFrequents_PH_V,50)
motsTresFrequents_PH_V <- findFreqTerms(dtm_PH_V, 199, Inf)
length(motsTresFrequents_PH_V)
head(motsTresFrequents_PH_V,50)
findAssocs(dtm_PH_V, terms = "hostis", corlimit = 0.7)
findAssocs(dtm_PH_V, terms = "rex", corlimit = 0.7)
findAssocs(dtm_PH_V, terms = "deus", corlimit = 0.7)
findAssocs(dtm_PH_V, terms = "murus", corlimit = 0.7)
rowTotals <- apply(dtm_PH_V, 1, sum)      #On trouve la somme des mots dans chaque document.
dtm_PH_V_clean   <- dtm_PH_V[rowTotals> 0, ]    #On retire tous les documents sans mot.
if(!require("topicmodels")){
install.packages("topicmodels")
library("topicmodels")
}
if(!require("ldatuning")){
install.packages("ldatuning")
library("ldatuning")
}
#Exécution du calcul avec la fonction FindTopicsNumber
topicsNumber_PH_V <- FindTopicsNumber(
#La DTM utilisée est la suivante :
dtm_PH_V_clean,
#Le nombre de possibilités testées :
topics = seq(from = 2, to = 20, by = 1),
#Les métriques utilisées
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE #Si c'est FALSE, cela supprime tous les avertissments et les informations additionnelles.
)
#Utilisation de la fonction seq()qui permet de créer une séquence d'éléments dans un vecteur. La syntaxe est la suivante : seq (from, to, by, length.out) from = élément de début de la séquence ; to = élément de fin de la séquence ; by = différence entre les éléments ; length.out = longueur maximale du vecteur.
#Affichage du résultat
FindTopicsNumber_plot(topicsNumber_PH_V)
#Lecture du graph : "“Griffiths” et “Deveaud” suivent un principe de maximisation alors que “CaoJuan” et “Arun” obéissent à un principe de minimisation. Je vous épargne les détails techniques, mais l’idée ici est d’identifier l’endroit où simultanément “Griffiths” et “Deveaud” se rejoignent le plus et où c’est également le cas pour “CaoJuan” et “Arun”. Tout est histoire de compromis, trouver l’endroit ou l’écart entre les courbes est minimal en haut et en bas !" (source : https://ouvrir.passages.cnrs.fr/wp-content/uploads/2019/07/rapp_topicmodel.html)
## Set parameters for Gibbs sampling
#Le modèle va tourner 2000 fois avant de commencer à enregistrer les résultats
burnin <- 2000
#Après cela il va encore tourner 2000 fois
iter <- 2000
# Il ne va enregistrer le résultat que toutes les 500 itérations
thin <- 500
#seed et nstart pour la reproductibilité
SEED=c(1, 2, 3, 4, 5)
seed <-SEED
nstart <- 5
#Seul le meilleur modèle est utilisé.
best <- TRUE
#8 topics
lda_gibbs_8_PH_V <- LDA(dtm_PH_V_clean, 8, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
#Utilisation de la fonction LDA avec la dtm utilisée, le nombre de topics, la méthode et le contrôle appliqué.
"LDA GIBBS 8"
termsTopic_lda_gibbs_8_PH_V <- as.data.frame(terms(lda_gibbs_8_PH_V,10))
head(termsTopic_lda_gibbs_8_PH_V,11)
topics_PH_V <- tidy(lda_gibbs_8_PH_V, matrix = "beta")
topics_PH_V
if (!require("dplyr")){
install.packages("dplyr")
library("dplyr")
}
if (!require("dplyr")){
install.packages("dplyr")
library("dplyr")
}
#Recupération des mots
top_terms_PH_V <- topics_PH_V %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup()  %>%
arrange(topic, -beta)
#Dessin du graphe
#On retrouve la fonction ggplot, cette fois-ci avec geom_col qui permet de créer des diagrammes à barres (barplots).
top_terms_PH_V %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
if (!require("reshape2")){
install.packages("reshape2")
library("reshape2")
}
df_PH_V_2 <- melt(as.matrix(dtm_PH_V_clean))
df_PH_V_2 <- df_PH_V_2[df_PH_V_2$Terms %in%findFreqTerms(dtm_PH_V_clean, lowfreq = 99), ]
ggplot(df_PH_V_2, aes(as.factor(Docs), Terms, fill=log(value))) +
geom_tile() +
xlab("Sujets") +
scale_fill_continuous(low="#FEE6CE", high="#E6550D") +
theme(axis.text.x = element_text(angle=90, hjust=1))
tt_PH_V <- posterior(lda_gibbs_8_PH_V)$terms
melted_PH_V = melt(tt_PH_V[,findFreqTerms(dtm_PH_V_clean, 75,500)])
colnames(melted_PH_V) <- c("Topics", "Terms", "value")
melted_PH_V$Topics <- as.factor(melted_PH_V$Topics)
ggplot(data = melted_PH_V, aes(x=Topics, y=Terms, fill=value)) +
geom_tile() +
theme(text = element_text(size=35))
DocumentTopicProbabilities_PH_V <- as.data.frame(lda_gibbs_8_PH_V@gamma)
rownames(DocumentTopicProbabilities_PH_V) <- rownames(corpus_PH_V)
head(DocumentTopicProbabilities_PH_V)
if (!require("wordcloud")){
install.packages("wordcloud")
library("wordcloud")
}
if (!require("RColorBrewer")){
install.packages("RColorBrewer")
library("RColorBrewer")
}
if (!require("wordcloud2")){
install.packages("wordcloud2")
library("wordcloud2")
}
tm_PH_V <- posterior(lda_gibbs_8_PH_V)$terms
data_PH_V = data.frame(colnames(tm_PH_V))
head(data_PH_V)
for(topic in seq(from = 1, to = 8, by = 1)){
data_PH_V$topic <-tm_PH_V[topic,]
#text(x=0.5, y=1, paste("V",topic, sep=""),cex=0.6)
wordcloud(
words = data_PH_V$colnames.tm_PH_V., #Mots à dessiner
freq = data_PH_V$topic, #Fréquence des mots
#Min.freq=sous ce seuil, les mots ne seront pas affichés
min.freq=0.0002,
#max.words=nombre maximum de mots à afficher
max.words=20,
#Random.order dessine les mots dans un ordre aléatoire. Si faux, ils sont dessinés par ordre décroissant de la fréquence.
random.order=FALSE,
#rot.per=% de mots à 90°
rot.per=.35,
#taille du graphe
scale=c(10,10),
#couleurs
colors = brewer.pal(5, "Dark2")
# il est possible de rentrer directement les couleurs qui nous intéressent
#c("red", "blue", "yellow", "chartreuse", "cornflowerblue", "darkorange")
)
}
setwd("~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe")
monDossier="~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe"
FJ_5_7 <- readLines("../../Textes/Flavius-Josephe/FJ_V-VII_lemm (1).txt")
FJ_5_7 <- readLines("../../Textes/Flavius_Josephe/FJ_V-VII_lemm (1).txt")
#Je charge deux nouvelles librairies pour le _text mining_ qui me permettent de créer ma matrice
if(!require("tm")){
install.packages("tm")
library("tm")
}
if(!require("tidytext")){
install.packages("tidytext")
library("tidytext")
}
# Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des données et des métadonnées
#La fonction `VectorSource` transforme chaque document en vecteur
corpus_FJ_5_7 <- Corpus(VectorSource(FJ_5_7), readerControl = list(language = "grc"))
# J'affiche les informations à propos de ce corpus
corpus_FJ_5_7
dtm_FJ_5_7 <- DocumentTermMatrix(corpus_FJ_5_7)
dtm_FJ_5_7
freq_FJ_5_7 <- as.data.frame(colSums(as.matrix(dtm_FJ_5_7)))
colnames(freq_FJ_5_7) <- c("frequence")
#as.data.frame est une fonction vérifiant qu'un objet est un dataframe ou le forçant à le devenir si c'est possible.
#colSums est une fonction permettant de former des sommes et des moyennes de lignes et de colonnes pour des tableaux et des dataframes.
#as.matrix est une fonction générique convertissant en matrice.
#colnames récupère ou définit le nom des lignes et des colonnes dans un objet de type matrice.
#c est une fonction générique qui combine ses arguments. La méthode par défaut combine les arguments pour former un vecteur.
#Pour dessiner un graphe, nécessité d'installer une nouvelle library: `ggplot2`
#gg = Grammar of Graphics
#Avec ggplot 2, les données représentées graphiquement proviennent toujours d'un dataframe.
if (!require("ggplot2")){
install.packages("ggplot2")
library("ggplot2")
}
#Dessin du graphe
#La fonction ggplot initialise le graphique. On commence par définir la source des données (ici freq_Vulgate), puis on indique quelle donnée on veut représenter (les attributs esthétiques) en passant des arguments dans la fonction aes(). Cette fonction spécifie les variables à visualiser et associe à chaque variable un emplacement ou un rôle: on renseigne le paramètre x qui est la variable à représenter sur l'axe horizontal (ici la fréquence).
#On ajoute, enfin, les éléments de représentation graphique (= geom). On les ajoute à l'objet graphique de base avec l'opérateur +. geom_density permet d'afficher l'estimation de densité d'une variable numérique. On crée une courbe de distribution.
#Source de la plupart des explications : https://juba.github.io/tidyverse/08-ggplot2.html
ggplot(freq_FJ_5_7, aes(x=frequence)) + geom_density()
motsPeuFrequents_FJ_5_7 <- findFreqTerms(dtm_FJ_5_7, 0, 99)
length(motsPeuFrequents_FJ_5_7)
head(motsPeuFrequents_FJ_5_7,50)
motsTresFrequents_FJ_5_7 <- findFreqTerms(dtm_FJ_5_7, 99, Inf)
length(motsTresFrequents_FJ_5_7)
head(motsTresFrequents_FJ_5_7,50)
motsTresFrequents_FJ_5_7 <- findFreqTerms(dtm_FJ_5_7, 199, Inf)
length(motsTresFrequents_FJ_5_7)
head(motsTresFrequents_FJ_5_7,50)
motsTresFrequents_FJ_5_7 <- findFreqTerms(dtm_FJ_5_7, 49, Inf)
length(motsTresFrequents_FJ_5_7)
head(motsTresFrequents_FJ_5_7,50)
rowTotals <- apply(dtm_FJ_5_7, 1, sum)      #On trouve la somme des mots dans chaque document.
dtm_FJ_5_7_clean   <- dtm_FJ_5_7[rowTotals> 0, ]    #On retire tous les documents sans mot.
if(!require("topicmodels")){
install.packages("topicmodels")
library("topicmodels")
}
if(!require("ldatuning")){
install.packages("ldatuning")
library("ldatuning")
}
#Exécution du calcul avec la fonction FindTopicsNumber
topicsNumber_FJ_5_7 <- FindTopicsNumber(
#La DTM utilisée est la suivante :
dtm_FJ_5_7_clean,
#Le nombre de possibilités testées :
topics = seq(from = 2, to = 20, by = 1),
#Les métriques utilisées
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE #Si c'est FALSE, cela supprime tous les avertissments et les informations additionnelles.
)
#Utilisation de la fonction seq()qui permet de créer une séquence d'éléments dans un vecteur. La syntaxe est la suivante : seq (from, to, by, length.out) from = élément de début de la séquence ; to = élément de fin de la séquence ; by = différence entre les éléments ; length.out = longueur maximale du vecteur.
#Affichage du résultat
FindTopicsNumber_plot(topicsNumber_FJ_5_7)
#Lecture du graph : "“Griffiths” et “Deveaud” suivent un principe de maximisation alors que “CaoJuan” et “Arun” obéissent à un principe de minimisation. Je vous épargne les détails techniques, mais l’idée ici est d’identifier l’endroit où simultanément “Griffiths” et “Deveaud” se rejoignent le plus et où c’est également le cas pour “CaoJuan” et “Arun”. Tout est histoire de compromis, trouver l’endroit ou l’écart entre les courbes est minimal en haut et en bas !" (source : https://ouvrir.passages.cnrs.fr/wp-content/uploads/2019/07/rapp_topicmodel.html)
## Set parameters for Gibbs sampling
#Le modèle va tourner 2000 fois avant de commencer à enregistrer les résultats
burnin <- 2000
#Après cela il va encore tourner 2000 fois
iter <- 2000
# Il ne va enregistrer le résultat que toutes les 500 itérations
thin <- 500
#seed et nstart pour la reproductibilité
SEED=c(1, 2, 3, 4, 5)
seed <-SEED
nstart <- 5
#Seul le meilleur modèle est utilisé.
best <- TRUE
#6 topics
lda_gibbs_FJ_5_7 <- LDA(dtm_FJ_5_7_clean, 9, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
#Utilisation de la fonction LDA avec la dtm utilisée, le nombre de topics, la méthode et le contrôle appliqué.
"LDA GIBBS 9"
termsTopic_lda_gibbs_FJ_5_7 <- as.data.frame(terms(lda_gibbs_FJ_5_7,9))
head(termsTopic_lda_gibbs_FJ_5_7,11)
topics_FJ_5_7 <- tidy(lda_gibbs_FJ_5_7, matrix = "beta")
topics_FJ_5_7
if (!require("dplyr")){
install.packages("dplyr")
library("dplyr")
}
#Recupération des mots
top_terms_FJ_5_7 <- topics_FJ_5_7 %>%
group_by(topic) %>%
top_n(9, beta) %>%
ungroup()  %>%
arrange(topic, -beta)
#Dessin du graphe
#On retrouve la fonction ggplot, cette fois-ci avec geom_col qui permet de créer des diagrammes à barres (barplots).
top_terms_FJ_5_7 %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
if (!require("reshape2")){
install.packages("reshape2")
library("reshape2")
}
if (!require("reshape2")){
install.packages("reshape2")
library("reshape2")
}
df2_FJ_5_7 <- melt(as.matrix(dtm_FJ_5_7_clean))
df2_FJ_5_7 <- df2_FJ_5_7[df2_FJ_5_7$Terms %in%findFreqTerms(dtm_FJ_5_7_clean, lowfreq = 100), ]
ggplot(df2_FJ_5_7, aes(as.factor(Docs), Terms, fill=log(value))) +
geom_tile() +
xlab("Sujets") +
scale_fill_continuous(low="#FEE6CE", high="#E6550D") +
theme(axis.text.x = element_text(angle=90, hjust=1))
tt_FJ_5_7 <- posterior(lda_gibbs_FJ_5_7)$terms
melted_FJ_5_7 = melt(tt_FJ_5_7[,findFreqTerms(dtm_FJ_5_7_clean, 75,500)])
colnames(melted_FJ_5_7) <- c("Topics", "Terms", "value")
melted_FJ_5_7$Topics <- as.factor(melted_FJ_5_7$Topics)
ggplot(data = melted_FJ_5_7, aes(x=Topics, y=Terms, fill=value)) +
geom_tile() +
theme(text = element_text(size=35))
DocumentTopicProbabilities_FJ_5_7 <- as.data.frame(lda_gibbs_FJ_5_7@gamma)
rownames(DocumentTopicProbabilities_FJ_5_7) <- rownames(corpus_FJ_5_7)
head(DocumentTopicProbabilities_FJ_5_7)
if (!require("wordcloud")){
install.packages("wordcloud")
library("wordcloud")
}
if (!require("RColorBrewer")){
install.packages("RColorBrewer")
library("RColorBrewer")
}
if (!require("wordcloud2")){
install.packages("wordcloud2")
library("wordcloud2")
}
tm_FJ_5_7 <- posterior(lda_gibbs_FJ_5_7)$terms
data_FJ_5_7 = data.frame(colnames(tm_FJ_5_7))
head(data_FJ_5_7)
for(topic in seq(from = 1, to = 9, by = 1)){
data_FJ_5_7$topic <-tm_FJ_5_7[topic,]
#text(x=0.5, y=1, paste("V",topic, sep=""),cex=0.6)
wordcloud(
words = data_FJ_5_7$colnames.tm_FJ_5_7., #Mots à dessiner
freq = data_FJ_5_7$topic, #Fréquence des mots
#Min.freq=sous ce seuil, les mots ne seront pas affichés
min.freq=0.0002,
#max.words=nombre maximum de mots à afficher
max.words=20,
#Random.order dessine les mots dans un ordre aléatoire. Si faux, ils sont dessinés par ordre décroissant de la fréquence.
random.order=FALSE,
#rot.per=% de mots à 90°
rot.per=.35,
#taille du graphe
scale=c(10,10),
#couleurs
colors = brewer.pal(5, "Dark2")
# il est possible de rentrer directement les couleurs qui nous intéressent
#c("red", "blue", "yellow", "chartreuse", "cornflowerblue", "darkorange")
)
}
setwd("~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe")
monDossier="~/Documents/GitHub/Topic_Modeling_compare_GuerreDesJuifs/Scripts/Pseudo-Hegesippe"
FJ_complet <- readLines("../../Textes/Flavius_Josephe/FJ_relu1.txt")
if(!require("tm")){
install.packages("tm")
library("tm")
}
if(!require("tidytext")){
install.packages("tidytext")
library("tidytext")
}
#Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des données et des métadonnées.
#La fonction `VectorSource` transforme chaque document en vecteur.
corpus_FJ_complet <- Corpus(VectorSource(FJ_complet), readerControl = list(language = "grc"))
# J'affiche les informations à propos de ce corpus
corpus_FJ_complet
dtm_FJ_complet <- DocumentTermMatrix(corpus_FJ_complet)
dtm_FJ_complet
freq_FJ_complet <- as.data.frame(colSums(as.matrix(dtm_FJ_complet)))
colnames(freq_FJ_complet) <- c("frequence")
#as.data.frame est une fonction vérifiant qu'un objet est un dataframe ou le forçant à le devenir si c'est possible.
#colSums est une fonction permettant de former des sommes et des moyennes de lignes et de colonnes pour des tableaux et des dataframes.
#as.matrix est une fonction générique convertissant en matrice.
#colnames récupère ou définit le nom des lignes et des colonnes dans un objet de type matrice.
#c est une fonction générique qui combine ses arguments. La méthode par défaut combine les arguments pour former un vecteur.
#Pour dessiner un graphe, nécessité d'installer une nouvelle library: `ggplot2`
#gg = Grammar of Graphics
#Avec ggplot 2, les données représentées graphiquement proviennent toujours d'un dataframe.
if (!require("ggplot2")){
install.packages("ggplot2")
library("ggplot2")
}
#Dessin du graphe
#La fonction ggplot initialise le graphique. On commence par définir la source des données (ici freq_Vulgate), puis on indique quelle donnée on veut représenter (les attributs esthétiques) en passant des arguments dans la fonction aes(). Cette fonction spécifie les variables à visualiser et associe à chaque variable un emplacement ou un rôle: on renseigne le paramètre x qui est la variable à représenter sur l'axe horizontal (ici la fréquence).
#On ajoute, enfin, les éléments de représentation graphique (= geom). On les ajoute à l'objet graphique de base avec l'opérateur +. geom_density permet d'afficher l'estimation de densité d'une variable numérique. On crée une courbe de distribution.
#Source de la plupart des explications : https://juba.github.io/tidyverse/08-ggplot2.html
ggplot(freq_FJ_complet, aes(x=frequence)) + geom_density()
motsPeuFrequents_FJ_complet <- findFreqTerms(dtm_FJ_complet, 0, 99)
length(motsPeuFrequents_FJ_complet)
head(motsPeuFrequents_FJ_complet,50)
motsTresFrequents_FJ_complet <- findFreqTerms(dtm_FJ_complet, 99, Inf)
#Si vous êts sur windows, décommentez la ligne suivante
#Encoding(motsTresFrequents)<-"latin-1"
length(motsTresFrequents_FJ_complet)
head(motsTresFrequents_FJ_complet,70)
motsTresFrequents_FJ_complet <- findFreqTerms(dtm_FJ_complet, 199, Inf)
#Si vous êts sur windows, décommentez la ligne suivante
#Encoding(motsTresFrequents)<-"latin-1"
length(motsTresFrequents_FJ_complet)
print(motsTresFrequents_FJ_complet)
findAssocs(dtm_FJ_complet, terms = "διαφθείρω", corlimit = 0.8)
findAssocs(dtm_FJ_complet, terms = "ναός", corlimit = 0.8)
rowTotals <- apply(dtm_FJ_complet, 1, sum)
dtm_FJ_complet_clean   <- dtm_FJ_complet[rowTotals> 0, ]
if(!require("topicmodels")){
install.packages("topicmodels")
library("topicmodels")
}
if(!require("ldatuning")){
install.packages("ldatuning")
library("ldatuning")
}
topicsNumber_FJ_complet <- FindTopicsNumber(
#On indique la DTM :
dtm_FJ_complet_clean,
#On indique le nombre de possibilité testées (cf. l'axe des abscisses qui commence à 2 et termine à 20) :
topics = seq(from = 2, to = 20, by = 1),
#Les métriques
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
#Affichage du résultat
FindTopicsNumber_plot(topicsNumber_FJ_complet)
#Lecture du graph : "“Griffiths” et “Deveaud” suivent un principe de maximisation alors que “CaoJuan” et “Arun” obéissent à un principe de minimisation. Je vous épargne les détails techniques, mais l’idée ici est d’identifier l’endroit où simultanément “Griffiths” et “Deveaud” se rejoignent le plus et où c’est également le cas pour “CaoJuan” et “Arun”. Tout est histoire de compromis, trouver l’endroit ou l’écart entre les courbes est minimal en haut et en bas !" (source : https://ouvrir.passages.cnrs.fr/wp-content/uploads/2019/07/rapp_topicmodel.html)
